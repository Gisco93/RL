\exercise{Reinforcement Learning}

\begin{questions}
	
%----------------------------------------------

\begin{question}{RL Exploration Strategies}{10}
	Discuss the two different exploration strategies applicable to RL.
	
\begin{answer}
	The two different exploration strategies in Reinforcement Learning are \textbf{exploitation} and \textbf{exploration}.\\ Exploiting means to maximize our reward with our current best action. This can be suboptimal when we find a local maxima or even dangerous when exploiting programmer mistakes like negative friction which might result in damage to our system.\\ Exploration means allowing for trying less attractive actions in hopes of finding other maxima that are better suited for the task at hand.\\ Finding the best possible trade-off between the two is a very hard task and is subject to a lot of discussion. One of the approaches discussed in the lecture is the $\epsilon$-greedy algorithm. This algorithm has a chance $\epsilon$ of choosing the action with the highest immediate reward or really close to it and otherwise choosing another action randomly. This results in an algorithm that uses both exploration and exploitation some amount of time which may lead to an optimal solution with a high probability. Furthermore we have the soft-max-policy which uses a continuous distribution over all actions using the Q-Function. This results in less harsh exploration and less harsh exploration but leads to good results as well.
\end{answer}
\end{question}


%----------------------------------------------


\end{questions}